{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('trainset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation=pd.read_csv('testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    #print(df[col].unique())\n",
    "    if df[col].isna().sum()>500:\n",
    "        df=df.drop(col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cls</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>nBase</th>\n",
       "      <th>SpAbs_A</th>\n",
       "      <th>SpMax_A</th>\n",
       "      <th>SpDiam_A</th>\n",
       "      <th>SpAD_A</th>\n",
       "      <th>SpMAD_A</th>\n",
       "      <th>LogEE_A</th>\n",
       "      <th>VE1_A</th>\n",
       "      <th>...</th>\n",
       "      <th>SRW10</th>\n",
       "      <th>TSRW10</th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WPath</th>\n",
       "      <th>WPol</th>\n",
       "      <th>Zagreb1</th>\n",
       "      <th>Zagreb2</th>\n",
       "      <th>mZagreb1</th>\n",
       "      <th>mZagreb2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.001104</td>\n",
       "      <td>2.501652</td>\n",
       "      <td>4.923679</td>\n",
       "      <td>25.001104</td>\n",
       "      <td>1.250055</td>\n",
       "      <td>3.907976</td>\n",
       "      <td>3.763026</td>\n",
       "      <td>...</td>\n",
       "      <td>9.918524</td>\n",
       "      <td>67.526731</td>\n",
       "      <td>284.075684</td>\n",
       "      <td>8.877365</td>\n",
       "      <td>764</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>7.777778</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.274271</td>\n",
       "      <td>2.409184</td>\n",
       "      <td>4.779725</td>\n",
       "      <td>20.274271</td>\n",
       "      <td>1.267142</td>\n",
       "      <td>3.699349</td>\n",
       "      <td>3.675755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.636588</td>\n",
       "      <td>61.986524</td>\n",
       "      <td>226.038985</td>\n",
       "      <td>9.827782</td>\n",
       "      <td>429</td>\n",
       "      <td>23</td>\n",
       "      <td>82</td>\n",
       "      <td>96</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>3.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.679070</td>\n",
       "      <td>2.528665</td>\n",
       "      <td>4.931112</td>\n",
       "      <td>23.679070</td>\n",
       "      <td>1.392886</td>\n",
       "      <td>3.823524</td>\n",
       "      <td>3.787841</td>\n",
       "      <td>...</td>\n",
       "      <td>10.022647</td>\n",
       "      <td>65.026165</td>\n",
       "      <td>223.074562</td>\n",
       "      <td>8.579791</td>\n",
       "      <td>461</td>\n",
       "      <td>29</td>\n",
       "      <td>98</td>\n",
       "      <td>121</td>\n",
       "      <td>3.416667</td>\n",
       "      <td>3.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.774585</td>\n",
       "      <td>2.557762</td>\n",
       "      <td>4.967131</td>\n",
       "      <td>27.774585</td>\n",
       "      <td>1.388729</td>\n",
       "      <td>4.000936</td>\n",
       "      <td>4.113451</td>\n",
       "      <td>...</td>\n",
       "      <td>10.250228</td>\n",
       "      <td>70.773493</td>\n",
       "      <td>266.069142</td>\n",
       "      <td>8.868971</td>\n",
       "      <td>735</td>\n",
       "      <td>35</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>3.888889</td>\n",
       "      <td>4.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.613943</td>\n",
       "      <td>2.477563</td>\n",
       "      <td>4.835703</td>\n",
       "      <td>39.613943</td>\n",
       "      <td>1.320465</td>\n",
       "      <td>4.331178</td>\n",
       "      <td>4.133872</td>\n",
       "      <td>...</td>\n",
       "      <td>10.233043</td>\n",
       "      <td>79.537169</td>\n",
       "      <td>415.183067</td>\n",
       "      <td>7.548783</td>\n",
       "      <td>2674</td>\n",
       "      <td>45</td>\n",
       "      <td>156</td>\n",
       "      <td>182</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>6.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25.162228</td>\n",
       "      <td>2.444418</td>\n",
       "      <td>4.888835</td>\n",
       "      <td>25.162228</td>\n",
       "      <td>1.324328</td>\n",
       "      <td>3.885650</td>\n",
       "      <td>3.788230</td>\n",
       "      <td>...</td>\n",
       "      <td>9.911108</td>\n",
       "      <td>52.566001</td>\n",
       "      <td>259.120843</td>\n",
       "      <td>7.197801</td>\n",
       "      <td>688</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>118</td>\n",
       "      <td>5.416667</td>\n",
       "      <td>4.194444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.266178</td>\n",
       "      <td>2.534987</td>\n",
       "      <td>4.934272</td>\n",
       "      <td>25.266178</td>\n",
       "      <td>1.203151</td>\n",
       "      <td>3.960779</td>\n",
       "      <td>3.746040</td>\n",
       "      <td>...</td>\n",
       "      <td>10.024155</td>\n",
       "      <td>69.315801</td>\n",
       "      <td>328.076075</td>\n",
       "      <td>8.633581</td>\n",
       "      <td>904</td>\n",
       "      <td>34</td>\n",
       "      <td>108</td>\n",
       "      <td>128</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>4.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.921252</td>\n",
       "      <td>2.296174</td>\n",
       "      <td>4.541085</td>\n",
       "      <td>23.921252</td>\n",
       "      <td>1.259013</td>\n",
       "      <td>3.850677</td>\n",
       "      <td>3.853332</td>\n",
       "      <td>...</td>\n",
       "      <td>9.488048</td>\n",
       "      <td>65.073826</td>\n",
       "      <td>278.094980</td>\n",
       "      <td>8.427121</td>\n",
       "      <td>813</td>\n",
       "      <td>23</td>\n",
       "      <td>92</td>\n",
       "      <td>103</td>\n",
       "      <td>6.305556</td>\n",
       "      <td>4.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.341681</td>\n",
       "      <td>2.541818</td>\n",
       "      <td>5.014881</td>\n",
       "      <td>35.341681</td>\n",
       "      <td>1.359295</td>\n",
       "      <td>4.226118</td>\n",
       "      <td>4.327006</td>\n",
       "      <td>...</td>\n",
       "      <td>10.356186</td>\n",
       "      <td>75.559148</td>\n",
       "      <td>352.169939</td>\n",
       "      <td>7.492977</td>\n",
       "      <td>1547</td>\n",
       "      <td>44</td>\n",
       "      <td>146</td>\n",
       "      <td>177</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.179816</td>\n",
       "      <td>2.416831</td>\n",
       "      <td>4.706367</td>\n",
       "      <td>31.179816</td>\n",
       "      <td>1.299159</td>\n",
       "      <td>4.102321</td>\n",
       "      <td>3.801567</td>\n",
       "      <td>...</td>\n",
       "      <td>9.878324</td>\n",
       "      <td>73.901759</td>\n",
       "      <td>361.077950</td>\n",
       "      <td>9.258409</td>\n",
       "      <td>1515</td>\n",
       "      <td>32</td>\n",
       "      <td>122</td>\n",
       "      <td>141</td>\n",
       "      <td>7.277778</td>\n",
       "      <td>5.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 1428 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cls  nAcid  nBase    SpAbs_A   SpMax_A  SpDiam_A     SpAD_A   SpMAD_A  \\\n",
       "0       1      0      0  25.001104  2.501652  4.923679  25.001104  1.250055   \n",
       "1       1      0      0  20.274271  2.409184  4.779725  20.274271  1.267142   \n",
       "2       1      0      0  23.679070  2.528665  4.931112  23.679070  1.392886   \n",
       "3       1      0      0  27.774585  2.557762  4.967131  27.774585  1.388729   \n",
       "4       1      0      0  39.613943  2.477563  4.835703  39.613943  1.320465   \n",
       "...   ...    ...    ...        ...       ...       ...        ...       ...   \n",
       "7995    0      0      1  25.162228  2.444418  4.888835  25.162228  1.324328   \n",
       "7996    0      0      0  25.266178  2.534987  4.934272  25.266178  1.203151   \n",
       "7997    0      2      0  23.921252  2.296174  4.541085  23.921252  1.259013   \n",
       "7998    0      0      0  35.341681  2.541818  5.014881  35.341681  1.359295   \n",
       "7999    0      0      0  31.179816  2.416831  4.706367  31.179816  1.299159   \n",
       "\n",
       "       LogEE_A     VE1_A  ...      SRW10     TSRW10          MW       AMW  \\\n",
       "0     3.907976  3.763026  ...   9.918524  67.526731  284.075684  8.877365   \n",
       "1     3.699349  3.675755  ...   9.636588  61.986524  226.038985  9.827782   \n",
       "2     3.823524  3.787841  ...  10.022647  65.026165  223.074562  8.579791   \n",
       "3     4.000936  4.113451  ...  10.250228  70.773493  266.069142  8.868971   \n",
       "4     4.331178  4.133872  ...  10.233043  79.537169  415.183067  7.548783   \n",
       "...        ...       ...  ...        ...        ...         ...       ...   \n",
       "7995  3.885650  3.788230  ...   9.911108  52.566001  259.120843  7.197801   \n",
       "7996  3.960779  3.746040  ...  10.024155  69.315801  328.076075  8.633581   \n",
       "7997  3.850677  3.853332  ...   9.488048  65.073826  278.094980  8.427121   \n",
       "7998  4.226118  4.327006  ...  10.356186  75.559148  352.169939  7.492977   \n",
       "7999  4.102321  3.801567  ...   9.878324  73.901759  361.077950  9.258409   \n",
       "\n",
       "      WPath  WPol  Zagreb1  Zagreb2  mZagreb1  mZagreb2  \n",
       "0       764    32      100      119  7.777778  4.666667  \n",
       "1       429    23       82       96  6.166667  3.583333  \n",
       "2       461    29       98      121  3.416667  3.638889  \n",
       "3       735    35      120      150  3.888889  4.166667  \n",
       "4      2674    45      156      182  8.500000  6.694444  \n",
       "...     ...   ...      ...      ...       ...       ...  \n",
       "7995    688    30      100      118  5.416667  4.194444  \n",
       "7996    904    34      108      128  9.250000  4.611111  \n",
       "7997    813    23       92      103  6.305556  4.361111  \n",
       "7998   1547    44      146      177  6.000000  5.583333  \n",
       "7999   1515    32      122      141  7.277778  5.416667  \n",
       "\n",
       "[8000 rows x 1428 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.index:\n",
    "    if df.loc[row].isna().sum()>0:\n",
    "        df.drop(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11424000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.cls.values\n",
    "#x=df.drop('cls',axis=1).drop('ABC',axis=1).drop('ABCGG',axis=1).drop('n5FAHRing',axis=1).values\n",
    "x=df.drop('cls',axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class dtset(Dataset):\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self,data,labels):\n",
    "        self.data=torch.FloatTensor(data)\n",
    "        self.labels=torch.FloatTensor(labels)\n",
    "    \n",
    "    # len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # obtain item\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder to reduce dimensionality\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, encoding_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request info on why ABC, ABCGG,nAcid,nBase are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,num_classes=2,drop_per=0.03):\n",
    "        \n",
    "        super(cnn,self).__init__()\n",
    "        \n",
    "        # 1st convolution\n",
    "        self.conv1d1 = nn.Conv1d(in_channels=input_size,out_channels=200,kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout1 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 2nd convolution \n",
    "        self.conv1d2 = nn.Conv1d(in_channels=200,out_channels=150,kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout2 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 3rd convolution\n",
    "        self.conv1d3 = nn.Conv1d(in_channels=150,out_channels=100,kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout3 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 4th convolution\n",
    "        self.conv1d4 = nn.Conv1d(in_channels=100,out_channels=50,kernel_size=3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout4 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 5th convolution\n",
    "        self.conv1d5 = nn.Conv1d(in_channels=50,out_channels=1,kernel_size=3)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1st convolution\n",
    "        x = self.conv1d1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 2nd convolution\n",
    "        x = self.conv1d2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 3rd convolution\n",
    "        x = self.conv1d3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # 4th convolution\n",
    "        x = self.conv1d4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # 5th convolution\n",
    "        x = self.conv1d5(x)\n",
    "        x = self.sigmoid1(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1427"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "\n",
    "# create dataset objects\n",
    "train=dtset(x_train,y_train)\n",
    "test=dtset(x_test,y_test)\n",
    "\n",
    "# create loaders\n",
    "train_loader=DataLoader(train,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6400, 1427), (6400,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame([i for i in train_loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600, 1427), (1600,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "encoding_dims=240\n",
    "# Init autoencoder\n",
    "ae=Autoencoder(input_size=size,encoding_dim=encoding_dims)\n",
    "\n",
    "# Init autoencoder loss func MSE\n",
    "ae_loss_func=nn.MSELoss()\n",
    "\n",
    "# Init autencoder optimzer adam\n",
    "ae_optimizer=optim.Adam(ae.parameters(),lr=0.0001 , eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "model=cnn(input_size=encoding_dims, num_classes=2, drop_per=0.03)\n",
    "\n",
    "# Init loss func crossentropy\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "\n",
    "# Init optimizer adam\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001, eps = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: nan\n",
      "Epoch [2/5], Loss: nan\n",
      "Epoch [3/5], Loss: nan\n",
      "Epoch [4/5], Loss: nan\n",
      "Epoch [5/5], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "ae_epochs=5\n",
    "\n",
    "# Run for epochs\n",
    "for epoch in range(ae_epochs):\n",
    "    # Run for items in loader\n",
    "    for data in train_loader:\n",
    "        inputs,_=data # ignore labels\n",
    "        ae_optimizer.zero_grad()\n",
    "        encoded,decoded=ae(inputs)\n",
    "        ae_loss=ae_loss_func(decoded,inputs)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{ae_epochs}], Loss: {ae_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [200, 240, 3], expected input[1, 16, 1427] to have 240 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m inputs,labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m     outputs\u001b[39m=\u001b[39mmodel(inputs)\n\u001b[1;32m      9\u001b[0m     loss\u001b[39m=\u001b[39mloss_func(outputs,labels)\n\u001b[1;32m     10\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mcnn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m     \u001b[39m# 1st convolution\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1d1(x)\n\u001b[1;32m     39\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[1;32m     40\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool1(x)\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [200, 240, 3], expected input[1, 16, 1427] to have 240 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs=10\n",
    "\n",
    "# Run for epochs\n",
    "for epoch in range(epochs):\n",
    "    # Run for items in loader\n",
    "    for inputs,labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_func(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "       \n",
    "# Save\n",
    "torch.save(ae.state_dict(),'models/autoencoder.pth')\n",
    "torch.save(model.state_dict(),'models/model.pth')\n",
    "\n",
    "# Load\n",
    "ae.load_state_dict(torch.load('models/autoencoder.pth'))\n",
    "model.load_state_dict(torch.load('models/model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
