{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('trainset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation=pd.read_csv('testset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop=df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of features with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td=todrop.sort_values(ascending=False)\n",
    "len(td[td.gt(0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of features with only NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(td[td==len(df)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request info on why ABC, ABCGG,nAcid,nBase are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    #print(df[col].unique())\n",
    "    if df[col].isna().sum()>500:\n",
    "        df=df.drop(col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7140, 1428)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.cls.values\n",
    "#x=df.drop('cls',axis=1).drop('ABC',axis=1).drop('ABCGG',axis=1).drop('n5FAHRing',axis=1).values\n",
    "x=df.drop('cls',axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class to create dataloaders which work well with pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class dtset(Dataset):\n",
    "    \n",
    "    # Init\n",
    "    def __init__(self,data,labels):\n",
    "        self.data=torch.FloatTensor(data)\n",
    "        self.labels=torch.FloatTensor(labels)\n",
    "    \n",
    "    # len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # obtain item\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autoencoder to reduce dimensionality of the data further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder to reduce dimensionality\n",
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size,int(input_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(input_size/2),int(input_size/4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(input_size/4),int(input_size/8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(input_size/8),encoding_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim,int(input_size/8)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(input_size/8),int(input_size/4)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(input_size/4),int(input_size/2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(input_size/2),input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CNN which will be our required model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,num_classes=2,drop_per=0.03):\n",
    "        \n",
    "        super(cnn,self).__init__()\n",
    "        \n",
    "        # 1st convolution\n",
    "        self.conv1d1 = nn.Conv1d(in_channels=input_size,out_channels=200,kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout1 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 2nd convolution \n",
    "        self.conv1d2 = nn.Conv1d(in_channels=200,out_channels=150,kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout2 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 3rd convolution\n",
    "        self.conv1d3 = nn.Conv1d(in_channels=150,out_channels=100,kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout3 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 4th convolution\n",
    "        self.conv1d4 = nn.Conv1d(in_channels=100,out_channels=50,kernel_size=3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.dropout4 = nn.Dropout(p=drop_per)\n",
    "        \n",
    "        # 5th convolution\n",
    "        self.conv1d5 = nn.Conv1d(in_channels=50,out_channels=1,kernel_size=3)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # 1st convolution\n",
    "        x = self.conv1d1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 2nd convolution\n",
    "        x = self.conv1d2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 3rd convolution\n",
    "        x = self.conv1d3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # 4th convolution\n",
    "        x = self.conv1d4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # 5th convolution\n",
    "        x = self.conv1d5(x)\n",
    "        x = self.sigmoid1(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1427"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "x_norm=scaler.fit_transform(x)\n",
    "y_norm=scaler.fit_transform(y.reshape(-1,1))#.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_norm,y_norm,test_size=0.2,random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "# create dataset objects\n",
    "train=dtset(x_train,y_train)\n",
    "test=dtset(x_test,y_test)\n",
    "\n",
    "# create loaders\n",
    "train_loader=DataLoader(train,batch_size=batch_size,shuffle=True)\n",
    "test_loader=DataLoader(test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5712, 1427), (5712, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1428, 1427), (1428, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=pd.DataFrame([i for i in train_loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "encoding_dims=batch_size\n",
    "# Init autoencoder\n",
    "ae=Autoencoder(input_size=size,encoding_dim=encoding_dims)\n",
    "\n",
    "# Init autoencoder loss func MSE\n",
    "ae_loss_func=nn.MSELoss()\n",
    "\n",
    "# Init autencoder optimzer adam\n",
    "ae_optimizer=optim.Adam(ae.parameters(),lr=0.001 , eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model\n",
    "model=cnn(input_size=encoding_dims, num_classes=2, drop_per=0.03)\n",
    "\n",
    "# Init loss func binary-crossentropy\n",
    "loss_func=nn.BCELoss()\n",
    "\n",
    "# Init optimizer adam\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001, eps = 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5608\n",
      "Epoch [2/20], Loss: 0.3719\n",
      "Epoch [3/20], Loss: 0.3899\n",
      "Epoch [4/20], Loss: 0.3637\n",
      "Epoch [5/20], Loss: 0.4014\n",
      "Epoch [6/20], Loss: 0.4159\n",
      "Epoch [7/20], Loss: 0.3194\n",
      "Epoch [8/20], Loss: 0.3345\n",
      "Epoch [9/20], Loss: 0.2679\n",
      "Epoch [10/20], Loss: 0.2372\n",
      "Epoch [11/20], Loss: 0.1847\n",
      "Epoch [12/20], Loss: 0.2850\n",
      "Epoch [13/20], Loss: 0.1880\n",
      "Epoch [14/20], Loss: 0.2483\n",
      "Epoch [15/20], Loss: 0.1983\n",
      "Epoch [16/20], Loss: 0.1329\n",
      "Epoch [17/20], Loss: 0.1451\n",
      "Epoch [18/20], Loss: 0.1952\n",
      "Epoch [19/20], Loss: 0.1359\n",
      "Epoch [20/20], Loss: 0.2124\n"
     ]
    }
   ],
   "source": [
    "ae_epochs=20\n",
    "\n",
    "# Run for epochs\n",
    "for epoch in range(ae_epochs):\n",
    "    # Run for items in loader\n",
    "    for data in train_loader:\n",
    "        inputs,_=data # ignore labels\n",
    "        ae_optimizer.zero_grad()\n",
    "        encoded,decoded=ae(inputs)\n",
    "        ae_loss=ae_loss_func(decoded,inputs)\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{ae_epochs}], Loss: {ae_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([1, 14])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ajdj/clones/cancer-detection/cnn.ipynb Cell 26\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ajdj/clones/cancer-detection/cnn.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ajdj/clones/cancer-detection/cnn.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs\u001b[39m=\u001b[39mmodel(inputs)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ajdj/clones/cancer-detection/cnn.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m=\u001b[39mloss_func(outputs,labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ajdj/clones/cancer-detection/cnn.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ajdj/clones/cancer-detection/cnn.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/mnt/stor/miniconda/envs/torch/lib/python3.11/site-packages/torch/nn/functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3092\u001b[0m     )\n\u001b[1;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([1, 14])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "\n",
    "epochs=10\n",
    "\n",
    "# Run for epochs\n",
    "for epoch in range(epochs):\n",
    "    # Run for items in loader\n",
    "    for inputs,labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(inputs)\n",
    "        loss=loss_func(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ae.state_dict(),'models/autoencoder.pth')\n",
    "torch.save(model.state_dict(),'models/model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.load_state_dict(torch.load('models/autoencoder.pth'))\n",
    "model.load_state_dict(torch.load('models/model.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
